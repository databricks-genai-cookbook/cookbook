{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a99d4119-ed8a-4cac-9b4f-dedf54349a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Using Databricks Vector Search with the Foundation Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940d9ef2-d01f-4e69-8f13-dc0203d4afd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks-vectorsearch\n  Using cached databricks_vectorsearch-0.22-py3-none-any.whl (8.5 kB)\nCollecting databricks-genai-inference\n  Using cached databricks_genai_inference-0.1.3-py3-none-any.whl (15 kB)\nCollecting mlflow-skinny<3,>=2.4.0\n  Using cached mlflow_skinny-2.9.2-py3-none-any.whl (4.7 MB)\nCollecting requests>=2\n  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\nCollecting protobuf<5,>=3.12.0\n  Using cached protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nCollecting pyyaml>=5.4.1\n  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\nCollecting pydantic>=2.4.2\n  Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\nCollecting typing-extensions>=4.7.1\n  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\nCollecting databricks-sdk>=0.12.0\n  Using cached databricks_sdk-0.18.0-py3-none-any.whl (439 kB)\nCollecting databricks-cli>=0.17.8\n  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\nCollecting pyjwt>=1.7.0\n  Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\nCollecting click>=7.0\n  Using cached click-8.1.7-py3-none-any.whl (97 kB)\nCollecting urllib3<3,>=1.26.7\n  Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\nCollecting six>=1.10.0\n  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nCollecting oauthlib>=3.1.0\n  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\nCollecting tabulate>=0.7.7\n  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\nCollecting google-auth~=2.0\n  Using cached google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\nCollecting entrypoints<1\n  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\nCollecting pytz<2024\n  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\nCollecting cloudpickle<4\n  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nCollecting sqlparse<1,>=0.4.0\n  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\nCollecting packaging<24\n  Using cached packaging-23.2-py3-none-any.whl (53 kB)\nCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\n  Using cached importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\nCollecting gitpython<4,>=2.1.0\n  Using cached GitPython-3.1.41-py3-none-any.whl (196 kB)\nCollecting pydantic-core==2.14.6\n  Using cached pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\nCollecting annotated-types>=0.4.0\n  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\nCollecting certifi>=2017.4.17\n  Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\nCollecting idna<4,>=2.5\n  Using cached idna-3.6-py3-none-any.whl (61 kB)\nCollecting charset-normalizer<4,>=2\n  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\nCollecting gitdb<5,>=4.0.1\n  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\nCollecting pyasn1-modules>=0.2.1\n  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\nCollecting cachetools<6.0,>=2.0.0\n  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\nCollecting rsa<5,>=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\nCollecting smmap<6,>=3.0.1\n  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\nCollecting pyasn1<0.6.0,>=0.4.6\n  Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\nInstalling collected packages: pytz, zipp, urllib3, typing-extensions, tabulate, sqlparse, smmap, six, pyyaml, pyjwt, pyasn1, protobuf, packaging, oauthlib, idna, entrypoints, cloudpickle, click, charset-normalizer, certifi, cachetools, annotated-types, rsa, requests, pydantic-core, pyasn1-modules, importlib-metadata, gitdb, pydantic, google-auth, gitpython, databricks-cli, mlflow-skinny, databricks-sdk, databricks-vectorsearch, databricks-genai-inference\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2023.3.post1\n    Uninstalling pytz-2023.3.post1:\n      Successfully uninstalled pytz-2023.3.post1\n  Attempting uninstall: zipp\n    Found existing installation: zipp 3.17.0\n    Uninstalling zipp-3.17.0:\n      Successfully uninstalled zipp-3.17.0\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.1.0\n    Uninstalling urllib3-2.1.0:\n      Successfully uninstalled urllib3-2.1.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: tabulate\n    Found existing installation: tabulate 0.9.0\n    Uninstalling tabulate-0.9.0:\n      Successfully uninstalled tabulate-0.9.0\n  Attempting uninstall: sqlparse\n    Found existing installation: sqlparse 0.4.4\n    Uninstalling sqlparse-0.4.4:\n      Successfully uninstalled sqlparse-0.4.4\n  Attempting uninstall: smmap\n    Found existing installation: smmap 5.0.1\n    Uninstalling smmap-5.0.1:\n      Successfully uninstalled smmap-5.0.1\n  Attempting uninstall: six\n    Found existing installation: six 1.16.0\n    Uninstalling six-1.16.0:\n      Successfully uninstalled six-1.16.0\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n  Attempting uninstall: pyjwt\n    Found existing installation: PyJWT 2.8.0\n    Uninstalling PyJWT-2.8.0:\n      Successfully uninstalled PyJWT-2.8.0\n  Attempting uninstall: pyasn1\n    Found existing installation: pyasn1 0.5.1\n    Uninstalling pyasn1-0.5.1:\n      Successfully uninstalled pyasn1-0.5.1\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.2\n    Uninstalling protobuf-4.25.2:\n      Successfully uninstalled protobuf-4.25.2\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.2\n    Uninstalling packaging-23.2:\n      Successfully uninstalled packaging-23.2\n  Attempting uninstall: oauthlib\n    Found existing installation: oauthlib 3.2.2\n    Uninstalling oauthlib-3.2.2:\n      Successfully uninstalled oauthlib-3.2.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: entrypoints\n    Found existing installation: entrypoints 0.4\n    Uninstalling entrypoints-0.4:\n      Successfully uninstalled entrypoints-0.4\n  Attempting uninstall: cloudpickle\n    Found existing installation: cloudpickle 3.0.0\n    Uninstalling cloudpickle-3.0.0:\n      Successfully uninstalled cloudpickle-3.0.0\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.3.2\n    Uninstalling charset-normalizer-3.3.2:\n      Successfully uninstalled charset-normalizer-3.3.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2023.11.17\n    Uninstalling certifi-2023.11.17:\n      Successfully uninstalled certifi-2023.11.17\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 5.3.2\n    Uninstalling cachetools-5.3.2:\n      Successfully uninstalled cachetools-5.3.2\n  Attempting uninstall: annotated-types\n    Found existing installation: annotated-types 0.6.0\n    Uninstalling annotated-types-0.6.0:\n      Successfully uninstalled annotated-types-0.6.0\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Uninstalling rsa-4.9:\n      Successfully uninstalled rsa-4.9\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.6\n    Uninstalling pydantic_core-2.14.6:\n      Successfully uninstalled pydantic_core-2.14.6\n  Attempting uninstall: pyasn1-modules\n    Found existing installation: pyasn1-modules 0.3.0\n    Uninstalling pyasn1-modules-0.3.0:\n      Successfully uninstalled pyasn1-modules-0.3.0\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 7.0.1\n    Uninstalling importlib-metadata-7.0.1:\n      Successfully uninstalled importlib-metadata-7.0.1\n  Attempting uninstall: gitdb\n    Found existing installation: gitdb 4.0.11\n    Uninstalling gitdb-4.0.11:\n      Successfully uninstalled gitdb-4.0.11\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: google-auth\n    Found existing installation: google-auth 2.27.0\n    Uninstalling google-auth-2.27.0:\n      Successfully uninstalled google-auth-2.27.0\n  Attempting uninstall: gitpython\n    Found existing installation: GitPython 3.1.41\n    Uninstalling GitPython-3.1.41:\n      Successfully uninstalled GitPython-3.1.41\n  Attempting uninstall: databricks-cli\n    Found existing installation: databricks-cli 0.18.0\n    Uninstalling databricks-cli-0.18.0:\n      Successfully uninstalled databricks-cli-0.18.0\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.9.2\n    Uninstalling mlflow-skinny-2.9.2:\n      Successfully uninstalled mlflow-skinny-2.9.2\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.18.0\n    Uninstalling databricks-sdk-0.18.0:\n      Successfully uninstalled databricks-sdk-0.18.0\n  Attempting uninstall: databricks-vectorsearch\n    Found existing installation: databricks-vectorsearch 0.22\n    Uninstalling databricks-vectorsearch-0.22:\n      Successfully uninstalled databricks-vectorsearch-0.22\n  Attempting uninstall: databricks-genai-inference\n    Found existing installation: databricks-genai-inference 0.1.3\n    Uninstalling databricks-genai-inference-0.1.3:\n      Successfully uninstalled databricks-genai-inference-0.1.3\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\nnbclassic 0.5.2 requires prometheus-client, which is not installed.\njupyter-server 1.23.4 requires prometheus-client, which is not installed.\ndatabricks-feature-store 0.16.3 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.2.0 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.3 which is incompatible.\ndeepspeed 0.11.1 requires pydantic<2.0.0, but you have pydantic 2.5.3 which is incompatible.\nbotocore 1.27.96 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.1.0 which is incompatible.\nSuccessfully installed annotated-types-0.6.0 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-3.0.0 databricks-cli-0.18.0 databricks-genai-inference-0.1.3 databricks-sdk-0.18.0 databricks-vectorsearch-0.22 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.41 google-auth-2.27.0 idna-3.6 importlib-metadata-7.0.1 mlflow-skinny-2.9.2 oauthlib-3.2.2 packaging-23.2 protobuf-4.25.2 pyasn1-0.5.1 pyasn1-modules-0.3.0 pydantic-2.5.3 pydantic-core-2.14.6 pyjwt-2.8.0 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 rsa-4.9 six-1.16.0 smmap-5.0.1 sqlparse-0.4.4 tabulate-0.9.0 typing-extensions-4.9.0 urllib3-2.1.0 zipp-3.17.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall databricks-vectorsearch databricks-genai-inference\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a67316b5-1a7c-485b-9828-57154049822b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "This will set up a temporary catalog/schema/table for this example. If you do not have permissions to create catalogs, you can specify a catalog manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cbf2ee-09d7-43cd-abfb-3d1cd4078e6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daniel_liden_databricks_com_20240125_232233_f4bec934\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Assuming DB_USER is fetched from the Databricks utility\n",
    "DB_USER = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "\n",
    "# Sanitize the DB_USER by replacing non-alphanumeric characters with underscores\n",
    "DB_USER_SANITIZED = re.sub(r'\\W', '_', DB_USER)\n",
    "\n",
    "# Append a timestamp and a UUID to ensure uniqueness\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "unique_id = str(uuid.uuid4()).split('-')[0]  # Get the first part of the UUID\n",
    "\n",
    "# Create a definitely unique DB_USER identifier\n",
    "DB_USER_UNIQUE = f\"{DB_USER_SANITIZED}_{timestamp}_{unique_id}\"\n",
    "\n",
    "print(DB_USER_UNIQUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5c55a7-17a2-4e4a-a922-85e25dbded12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = DB_USER_UNIQUE\n",
    "DB = \"FM_API_EXAMPLES\"\n",
    "VS_ENDPOINT_NAME = \"test_endpoint\"\n",
    "VS_INDEX_NAME = \"FM_API_EXAMPLES_VS_INDEX\"\n",
    "SOURCE_TABLE_NAME = \"FM_API_EXAMPLES_DATA\"\n",
    "\n",
    "VS_INDEX_FULLNAME = f\"{CATALOG}.{DB}.{VS_INDEX_NAME}\"\n",
    "SOURCE_TABLE_FULLNAME = f\"{CATALOG}.{DB}.{SOURCE_TABLE_NAME}\"\n",
    "DATABRICKS_TOKEN = dbutils.secrets.get(scope=\"daniel.liden\", key=\"rag_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4dbac4b-2852-4b49-9e43-f514aca70a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up schema/volume/table\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{DB}\")\n",
    "spark.sql(\n",
    "    f\"\"\"CREATE TABLE IF NOT EXISTS {SOURCE_TABLE_FULLNAME} (\n",
    "        id STRING,\n",
    "        text STRING,\n",
    "        date DATE,\n",
    "        title STRING\n",
    "    )\n",
    "    USING delta \n",
    "    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79471a4-27c4-4b18-90d0-cce297dbca1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE {SOURCE_TABLE_FULLNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3348acab-c299-4d2f-b97a-c49affc1abd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604edff9-24cc-484d-bfd9-002dc820ebfc",
     "showTitle": true,
     "title": "Python Delta Sync Index Setup"
    }
   },
   "outputs": [],
   "source": [
    "# set up an index with managed embeddings\n",
    "i=vsc.create_delta_sync_index(\n",
    "    endpoint_name=VS_ENDPOINT_NAME,\n",
    "    index_name=VS_INDEX_FULLNAME,\n",
    "    source_table_name=SOURCE_TABLE_FULLNAME,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    primary_key=\"id\",\n",
    "    embedding_source_column=\"text\",\n",
    "    embedding_model_endpoint_name=\"databricks-bge-large-en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b54ae9-fe2d-4c94-9ee7-34921ee43476",
     "showTitle": true,
     "title": "Smart Initiative: Strategic Management for Achieving Results through Efficiency and Resources"
    }
   },
   "outputs": [],
   "source": [
    "# Some fake texts\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "smarter_overview = {\"text\":\"\"\"\n",
    "S.M.A.R.T.E.R. Initiative: Strategic Management for Achieving Results through Efficiency and Resources\n",
    "Introduction\n",
    "The S.M.A.R.T.E.R. Initiative, standing for \"Strategic Management for Achieving Results through Efficiency and Resources,\" is a groundbreaking project aimed at revolutionizing the way our organization operates. In today's rapidly changing business landscape, achieving success demands a strategic approach that leverages resources effectively while optimizing efficiency. The S.M.A.R.T.E.R. Initiative is designed to do just that.\n",
    "\n",
    "Background\n",
    "As markets evolve and competition intensifies, organizations must adapt to stay relevant and profitable. Traditional methods of operation often become inefficient and costly. The S.M.A.R.T.E.R. Initiative was conceived as a response to this challenge, with the primary goal of enhancing strategic management practices to achieve better results.\n",
    "\n",
    "Objectives\n",
    "1. Resource Optimization\n",
    "One of the key objectives of the S.M.A.R.T.E.R. Initiative is to optimize resource allocation. This involves identifying underutilized resources, streamlining processes, and reallocating resources to areas that contribute most to our strategic goals.\n",
    "\n",
    "2. Efficiency Improvement\n",
    "Efficiency is at the core of the S.M.A.R.T.E.R. Initiative. By identifying bottlenecks and improving processes, we aim to reduce operational costs, shorten project timelines, and enhance overall productivity.\n",
    "\n",
    "3. Strategic Alignment\n",
    "For any organization to succeed, its activities must be aligned with its strategic objectives. The S.M.A.R.T.E.R. Initiative will ensure that every action and resource allocation is in sync with our long-term strategic goals.\n",
    "\n",
    "4. Results-driven Approach\n",
    "The ultimate measure of success is results. The S.M.A.R.T.E.R. Initiative will foster a results-driven culture within our organization, where decisions and actions are guided by their impact on our bottom line and strategic objectives.\n",
    "\n",
    "Key Components\n",
    "The S.M.A.R.T.E.R. Initiative comprises several key components:\n",
    "\n",
    "1. Data Analytics and Insights\n",
    "Data is the foundation of informed decision-making. We will invest in advanced data analytics tools to gain insights into our operations, customer behavior, and market trends. These insights will guide our resource allocation and strategy.\n",
    "\n",
    "2. Process Automation\n",
    "Automation will play a vital role in enhancing efficiency. Routine and repetitive tasks will be automated, freeing up our workforce to focus on more strategic activities.\n",
    "\n",
    "3. Performance Metrics and KPIs\n",
    "To ensure that our efforts are aligned with our objectives, we will establish a comprehensive set of Key Performance Indicators (KPIs). Regular monitoring and reporting will provide visibility into our progress.\n",
    "\n",
    "4. Training and Development\n",
    "Enhancing our workforce's skills is essential. We will invest in training and development programs to equip our employees with the knowledge and tools needed to excel in their roles.\n",
    "\n",
    "Implementation Timeline\n",
    "The S.M.A.R.T.E.R. Initiative will be implemented in phases over the next three years. This phased approach allows for a smooth transition and ensures that each component is integrated effectively into our operations.\n",
    "\n",
    "Conclusion\n",
    "The S.M.A.R.T.E.R. Initiative represents a significant step forward for our organization. By strategically managing our resources and optimizing efficiency, we are positioning ourselves for sustained success in a competitive marketplace. This initiative is a testament to our commitment to excellence and our dedication to achieving exceptional results.\n",
    "\n",
    "As we embark on this journey, we look forward to the transformative impact that the S.M.A.R.T.E.R. Initiative will have on our organization and the benefits it will bring to our employees, customers, and stakeholders.\n",
    "\"\"\", \"title\": \"Project Kickoff\", \"date\": datetime.strptime(\"2024-01-16\", \"%Y-%m-%d\")}\n",
    "\n",
    "smarter_kpis = {\"text\": \"\"\"S.M.A.R.T.E.R. Initiative: Key Performance Indicators (KPIs)\n",
    "Introduction\n",
    "The S.M.A.R.T.E.R. Initiative (Strategic Management for Achieving Results through Efficiency and Resources) is designed to drive excellence within our organization. To measure the success and effectiveness of this initiative, we have established three concrete and measurable Key Performance Indicators (KPIs). This document outlines these KPIs and their associated targets.\n",
    "\n",
    "Key Performance Indicators (KPIs)\n",
    "1. Resource Utilization Efficiency (RUE)\n",
    "Objective: To optimize resource utilization for cost-efficiency.\n",
    "\n",
    "KPI Definition: RUE will be calculated as (Actual Resource Utilization / Planned Resource Utilization) * 100%.\n",
    "\n",
    "Target: Achieve a 15% increase in RUE within the first year.\n",
    "\n",
    "2. Time-to-Decision Reduction (TDR)\n",
    "Objective: To streamline operational processes and reduce decision-making time.\n",
    "\n",
    "KPI Definition: TDR will be calculated as (Pre-Initiative Decision Time - Post-Initiative Decision Time) / Pre-Initiative Decision Time.\n",
    "\n",
    "Target: Achieve a 20% reduction in TDR for critical business decisions.\n",
    "\n",
    "3. Strategic Goals Achievement (SGA)\n",
    "Objective: To ensure that organizational activities align with strategic goals.\n",
    "\n",
    "KPI Definition: SGA will measure the percentage of predefined strategic objectives achieved.\n",
    "\n",
    "Target: Achieve an 80% Strategic Goals Achievement rate within two years.\n",
    "\n",
    "Conclusion\n",
    "These three KPIs, Resource Utilization Efficiency (RUE), Time-to-Decision Reduction (TDR), and Strategic Goals Achievement (SGA), will serve as crucial metrics for evaluating the success of the S.M.A.R.T.E.R. Initiative. By tracking these KPIs and working towards their targets, we aim to drive efficiency, optimize resource utilization, and align our actions with our strategic objectives. This focus on measurable outcomes will guide our efforts towards achieving excellence within our organization.\"\"\",\n",
    "\"title\": \"Project KPIs\", \"date\": datetime.strptime(\"2024-01-16\", \"%Y-%m-%d\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb4b55d-2935-4571-96e1-eec742466327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(words):\n",
    "        end = index + chunk_size\n",
    "        while end < len(words) and not re.match(r'.*[.!?]\\s*$', words[end]):\n",
    "            end += 1\n",
    "        chunk = ' '.join(words[index:end+1])\n",
    "        chunks.append(chunk)\n",
    "        index += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Use the function\n",
    "chunks = []\n",
    "documents = [smarter_overview, smarter_kpis]  # Replace with your actual documents\n",
    "\n",
    "for document in documents:\n",
    "    for i, c in enumerate(chunk_text(document[\"text\"], 150, 25)):\n",
    "        chunk = {}\n",
    "        chunk[\"text\"] = c\n",
    "        chunk[\"title\"] = document[\"title\"]\n",
    "        chunk[\"date\"] = document[\"date\"]\n",
    "        chunk[\"id\"] = document[\"title\"] + \"_\" + str(i)\n",
    "\n",
    "        chunks.append(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b8a7ba-32eb-4829-a560-1dd210ede177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, DateType\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if chunks:\n",
    "    result_df = spark.createDataFrame(chunks, schema=schema)\n",
    "    result_df.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "        SOURCE_TABLE_FULLNAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef5b5b3-7105-4e5b-a604-c7994be6df61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sync\n",
    "index = vsc.get_index(endpoint_name=VS_ENDPOINT_NAME,\n",
    "                      index_name=VS_INDEX_FULLNAME)\n",
    "index.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c088bddc-48b7-46ab-ad8d-7072883923a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'manifest': {'column_count': 2,\n",
       "  'columns': [{'name': 'text'}, {'name': 'score'}]},\n",
       " 'result': {'row_count': 3,\n",
       "  'data_array': [['S.M.A.R.T.E.R. Initiative: Key Performance Indicators (KPIs) Introduction The S.M.A.R.T.E.R. Initiative (Strategic Management for Achieving Results through Efficiency and Resources) is designed to drive excellence within our organization. To measure the success and effectiveness of this initiative, we have established three concrete and measurable Key Performance Indicators (KPIs). This document outlines these KPIs and their associated targets. Key Performance Indicators (KPIs) 1. Resource Utilization Efficiency (RUE) Objective: To optimize resource utilization for cost-efficiency. KPI Definition: RUE will be calculated as (Actual Resource Utilization / Planned Resource Utilization) * 100%. Target: Achieve a 15% increase in RUE within the first year. 2. Time-to-Decision Reduction (TDR) Objective: To streamline operational processes and reduce decision-making time. KPI Definition: TDR will be calculated as (Pre-Initiative Decision Time - Post-Initiative Decision Time) / Pre-Initiative Decision Time. Target: Achieve a 20% reduction in TDR for critical business decisions. 3. Strategic Goals Achievement (SGA) Objective: To ensure that organizational activities align with strategic goals.',\n",
       "    0.5684492],\n",
       "   ['Time) / Pre-Initiative Decision Time. Target: Achieve a 20% reduction in TDR for critical business decisions. 3. Strategic Goals Achievement (SGA) Objective: To ensure that organizational activities align with strategic goals. KPI Definition: SGA will measure the percentage of predefined strategic objectives achieved. Target: Achieve an 80% Strategic Goals Achievement rate within two years. Conclusion These three KPIs, Resource Utilization Efficiency (RUE), Time-to-Decision Reduction (TDR), and Strategic Goals Achievement (SGA), will serve as crucial metrics for evaluating the success of the S.M.A.R.T.E.R. Initiative. By tracking these KPIs and working towards their targets, we aim to drive efficiency, optimize resource utilization, and align our actions with our strategic objectives. This focus on measurable outcomes will guide our efforts towards achieving excellence within our organization.',\n",
       "    0.5681083],\n",
       "   ['S.M.A.R.T.E.R. Initiative is to optimize resource allocation. This involves identifying underutilized resources, streamlining processes, and reallocating resources to areas that contribute most to our strategic goals. 2. Efficiency Improvement Efficiency is at the core of the S.M.A.R.T.E.R. Initiative. By identifying bottlenecks and improving processes, we aim to reduce operational costs, shorten project timelines, and enhance overall productivity. 3. Strategic Alignment For any organization to succeed, its activities must be aligned with its strategic objectives. The S.M.A.R.T.E.R. Initiative will ensure that every action and resource allocation is in sync with our long-term strategic goals. 4. Results-driven Approach The ultimate measure of success is results. The S.M.A.R.T.E.R. Initiative will foster a results-driven culture within our organization, where decisions and actions are guided by their impact on our bottom line and strategic objectives. Key Components The S.M.A.R.T.E.R. Initiative comprises several key components: 1. Data Analytics and Insights Data is the foundation of informed decision-making.',\n",
       "    0.55466974]]},\n",
       " 'next_page_token': '',\n",
       " 'debug_info': {'response_time': 243.0,\n",
       "  'ann_time': 22.0,\n",
       "  'embedding_gen_time': 141.0}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query\n",
    "index.similarity_search(columns=[\"text\"],\n",
    "                        query_text=\"What is the TDR Target for the SMARTER initiative?\",\n",
    "                        num_results = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ffe039-7033-4c4f-92d6-38cf1cc44178",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleanup\n",
    "The code snippet below will delete the VS index and source table, along with the catalog and schema if they are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef39e064-deca-4eff-82d4-09a37af525ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # delete index\n",
    "vsc.delete_index(endpoint_name=VS_ENDPOINT_NAME,\n",
    "                  index_name=VS_INDEX_FULLNAME)\n",
    "\n",
    "# delete schema and catalog\n",
    "spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Using Databricks Vector Search with the Foundation Model API",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
