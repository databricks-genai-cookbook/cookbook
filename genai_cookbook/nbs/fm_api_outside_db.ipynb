{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(nbs:fm_api_outside_db)=\n",
    "# Using the Foundation Model API Outside of Databricks\n",
    "\n",
    "## Introduction\n",
    "While Databricks provides a great environment for working with AI tools including the Foundation Model API, you can easily call on the API from outside of Databricks. This is useful if you want to use the Foundation Model API in your app or while writing code in another editor such as VSCode, Vim, or Emacs. This notebook shows how to do so. The workflow will be familiar to users of other AI inference products such as those offered by OpenAI or Anthropic: you need to generate an API key and pass it to the API calls in order to query the models. Additionally, you will need the URL of your Databricks workspace.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### 1. Generate a Personal Access Token (PAT)\n",
    "First, generate a [personal access token](https://docs.databricks.com/en/dev-tools/auth/pat.html);\n",
    "1. In your workspace, navigate to \"User Settings\" in the dropdown menu on the top right of the screen.\n",
    "2. Click \"Developer\" in the menu on the left, and then click \"Manage\" under \"Access Tokens.\"\n",
    "3. Click \"Generate new token\" to create a new PAT.\n",
    "4. Copy the token and paste it to a secure location such as a `.env` file.\n",
    "\n",
    "```{warning}\n",
    "If you are tracking your code with git, make sure to add this file to your `.gitignore` file to avoid inadvertently sharing it with others.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "If you lose your PAT, you will need to generate a new one. You cannot access the PAT from this menu again.\n",
    "```\n",
    "\n",
    "### 2. Identify your Databricks host URL\n",
    "To use the Foundation Model API, you need to identify the URL of your Databricks workspace. The URL typically starts with `https://` and includes your Databricks [instance name](https://docs.databricks.com/en/workspace/workspace-details.html#workspace-instance-names-urls-and-ids). `https://cust-success.cloud.databricks.com/` is an example of what the URL might look like.\n",
    "\n",
    "## Query the API with the Python SDK\n",
    "\n",
    "Once you have obtained your PAT and Databricks host URL, you can query the API with the Python SDK.\n",
    "\n",
    "### Install the SDK\n",
    "\n",
    "You can [install the Python SDK](https://docs.databricks.com/en/machine-learning/foundation-models/query-foundation-model-apis.html#install-the-sdk-on-your-local-environment) with pip:\n",
    "\n",
    "```sh\n",
    "pip install databricks-genai-inference\n",
    "```\n",
    "\n",
    "### Query\n",
    "\n",
    "Now you can query the API with the Python SDK. This is much the same as using the Python SDK in a Databricks notebook except that you need to provide the `databricks_host` and `databricks_token` parameters. There are a number of different ways to do this. A couple of common patterns for handling these parameters include:\n",
    "\n",
    "**export the environment variables:**\n",
    "\n",
    "This will make the `DATABRICKS_HOST` and `DATABRICKS_TOKEN` environment variables available to your code. You will not need to pass them explicitly to your API calls.\n",
    "\n",
    "```sh\n",
    "export DATABRICKS_HOST=\"https://<instance_name>.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"<your_personal_access_token>\"\n",
    "```\n",
    "\n",
    "**load the variables from a `.env` file:**\n",
    "\n",
    "You can load the variables from a `.env` file using the `load_dotenv` function from the [`Python-dotenv`](https://pypi.org/project/python-dotenv/) library.\n",
    "\n",
    "The `.env` file should look something like this:\n",
    "\n",
    "```\n",
    "DATABRICKS_HOST=\"https://<instance_name>.cloud.databricks.com\"\n",
    "DATABRICKS_TOKEN=\"<your_personal_access_token>\"\n",
    "```\n",
    "\n",
    "Remember: do not commit this file to your git project; add it to your `.gitignore` file to avoid sharing your credentials with others.\n",
    "\n",
    "You can load the variables from your `.env` file in python as follows:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv # for loading environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the hostname and token from the `.env` file are accessible to the Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks_genai_inference import ChatCompletion\n",
    "\n",
    "response = ChatCompletion.create(model=\"llama-2-70b-chat\",\n",
    "                                 messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                           {\"role\": \"user\",\"content\": \"Knock knock.\"}],\n",
    "                                 max_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWho's there? I'm happy to help with anything you need!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the REST API with curl\n",
    "\n",
    "To query the REST API with `curl`, we again must be mindful of the host and token. Requests are structured as follows. This expects that the `DATABRICKS_TOKEN` and `DATABRICKS_HOST` environment variables are set. As a reminder, you can do this with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DATABRICKS_HOST=\"https://<instance_name>.cloud.databricks.com\"\n",
    "export DATABRICKS_TOKEN=\"<your_personal_access_token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these variables set, you can query the REST API with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100   851  100   494  100   357    245    177  0:00:02  0:00:02 --:--:--   422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"6f7b2001-583b-4554-9bd6-80a8984f64c4\",\"object\":\"chat.completion\",\"created\":1707860729,\"model\":\"llama-2-70b-chat\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\\nMLflow autologging is a feature that automatically logs and tracks experiments, runs, and models in a centralized database, providing a seamless way to manage and track the end-to-end machine learning lifecycle.\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":44,\"completion_tokens\":48,\"total_tokens\":92}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -u token:$DATABRICKS_TOKEN \\\n",
    "     -X POST \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "           \"messages\": [\n",
    "             {\n",
    "               \"role\": \"system\",\n",
    "               \"content\": \"You are a helpful assistant. Keep your responses short and concise.\"\n",
    "             },\n",
    "             {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": \"What is MLflow autologging?\"\n",
    "             }\n",
    "           ],\n",
    "           \"max_tokens\": 128\n",
    "         }' \\\n",
    "     $DATABRICKS_HOST/serving-endpoints/databricks-llama-2-70b-chat/invocations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has shown the basics of using the Foundation Model API outside of a Databricks environment. You can now start using the API to build AI applications or to develop other projects in the development environment of your choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
